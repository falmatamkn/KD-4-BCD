# Resource Usage Experiments ‚Äî Breast Cancer Detection Models

This directory contains the complete setup used to evaluate **resource usage, latency, and deployability** of the models described in the paper:

**"Development of a Knowledge-Distillation-Based Breast Cancer Classifier for LMICs: Comparison with Pruning and Quantization."**

The experiments measure **inference latency, CPU and memory overhead, power consumption, and temperature** of the baseline Teacher Model (TM), pruned and quantized variants, and the Knowledge-Distilled Student Model (SM) on a **Raspberry Pi 4 Model B**.

Each subfolder corresponds to a specific compression technique evaluated in Section 6 of the paper.

---

## üìÅ Folder Structure Overview

### `KD/`
Contains the Jupyter notebook used to evaluate the deployability of the **Student Model (SM)** and its TensorFlow Lite version **SM_TFLite**.  
Includes inference experiments, resource monitoring, and power measurements for models trained via Knowledge Distillation.

---

### `Pruning/`
Contains evaluation code for the magnitude-pruned Teacher Model (**TM_Prune**).

Contents include:
- `evaluating_TM_Pruning.py` ‚Äî Script that loads TM_Prune, runs inference, and reports latency  
- `sys_mon.py` ‚Äî System monitor for CPU, RAM, and temperature logging  
- Experiment notebook (`.ipynb`)
- `files/` ‚Äî Contains Keras and TFLite versions of TM_Prune  
- `results/` ‚Äî Stores logs, latency data, and resource-usage outputs  

---

### `PTQ/`
Contains resource-usage experiments for **Post-Training Quantization**, including:
- Weight-only quantization (**TM_PTQ**)  
- Full-integer quantization (**TM_PTQ_INT**)  

Each folder includes:
- Python inference scripts  
- System monitoring script  
- Full experiment notebook  
- `files/` folder with all model artifacts  
- `results/` directory for logs and graphs  

---

### `QAT/`
Contains evaluation code for **Quantization-Aware Training (TM_QAT)**.

Includes:
- Inference script  
- System monitoring script  
- Notebook containing the full QAT deployability workflow  
- `files/` ‚Äî Keras and TFLite model files  
- `results/` ‚Äî Performance logs and measurements  

---

## üìÑ Common Files in Each Experiment Folder

### **Jupyter Notebooks (`*.ipynb`)**
Each notebook performs the complete deployability evaluation:
- Loads the model  
- Loads test samples  
- Measures inference latency  
- Runs resource monitoring  
- Collects CPU, memory, temperature, and power traces  
- Saves plots and logs to `results/`

### **`files/` Directory**
Contains:
- Trained **Keras (.keras / .h5)** models  
- Optimized **TFLite (.tflite)** models  
- Any intermediate conversion models  

### **`results/` Directory**
Stores all outputs generated by the notebook:
- Timing logs  
- CPU/RAM usage logs  
- Temperature logs  
- Power consumption traces  
- Visual plots and graphs  

### **Evaluation Scripts (e.g., `evaluating_TM.py`)**
Scripts that run inference and print timing measurements such as:
- Load model time
- Get input/output tensor time
- Prepare input data time
- Single-sample inference time


These follow the structure shown in `evaluating_TM.py`.

---

## üõ† System Monitor (`sys_mon.py`)
This script continuously logs:
- CPU usage (%)  
- Memory usage (%)  
- CPU temperature (¬∞C)  

Values are stored each second into an output text file, enabling later plotting and analysis.

---

## üìå Notes
- All experiments run on **Raspberry Pi 4B (8GB)** with **Raspbian OS 11**.  
- All models executed in **CPU-only mode**.  
- TensorFlow Lite automatically used the **XNNPACK CPU delegate**.  
- Power measurements used the Arduino + ACS712 setup described in the paper.

---

This README describes all resource-usage experiment folders and their contents to ensure full reproducibility of the Raspberry Pi deployability tests.
